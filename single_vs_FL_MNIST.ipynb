{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.8.1-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python38164bitpysyftcondae654683451e644f196d2539ccb886504",
   "display_name": "Python 3.8.1 64-bit ('pysyft': conda)"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as th\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torch import nn, optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,),(0.5,))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data\\MNIST\\raw\\train-images-idx3-ubyte.gz\n100.1%Extracting ./data\\MNIST\\raw\\train-images-idx3-ubyte.gz to ./data\\MNIST\\raw\nDownloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\\train-labels-idx1-ubyte.gz\n113.5%Extracting ./data\\MNIST\\raw\\train-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\nDownloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz\n100.4%Extracting ./data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz to ./data\\MNIST\\raw\nDownloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz\n180.4%Extracting ./data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\nProcessing...\nDone!\n"
    }
   ],
   "source": [
    "mnist_trainset = datasets.MNIST(root=\"./data\",train=True, download=True, transform = transform)\n",
    "trainloader = th.utils.data.DataLoader(mnist_trainset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "testloader = th.utils.data.DataLoader(\n",
    "    datasets.MNIST('./data',train=False, transform=transforms.Compose([\n",
    "        transforms .ToTensor(),\n",
    "        transforms.Normalize((0.5,),(0.5,))\n",
    "    ])),\n",
    "    batch_size=64, shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "Dataset MNIST\n    Number of datapoints: 60000\n    Root location: ./data\n    Split: Train\n    StandardTransform\nTransform: Compose(\n               ToTensor()\n               Normalize(mean=(0.5,), std=(0.5,))\n           )"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainloader.dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Arguments():\n",
    "    def __init__(self):\n",
    "        self.batch_size = 64\n",
    "        self.test_batch_size = 1000\n",
    "        self.epochs = 5\n",
    "        self.lr = 0.01\n",
    "        self.momentum = 0.5\n",
    "        self.no_cuda = False\n",
    "        self.seed = 1\n",
    "        self.log_interval = 30\n",
    "        self.save_model = False\n",
    "\n",
    "args = Arguments()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([28, 28])"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainloader.dataset.data[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainSingle(args, model, train_loader, epoch):\n",
    "    \n",
    "    criterion = nn.NLLLoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.003)\n",
    "\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model(data.view(data.shape[0], -1))\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch_idx % args.log_interval == 0:\n",
    "\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * args.batch_size, len(train_loader)*args.batch_size,\n",
    "                100. * batch_idx / len(train_loader), loss.item()\n",
    "            ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(args, model, test_loader):\n",
    "    \n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    criterion = nn.NLLLoss()\n",
    "\n",
    "    with th.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            output = model(data.view(data.shape[0], -1))\n",
    "            test_loss += criterion(output, target).item()\n",
    "            pred = output.argmax(1, keepdim=True)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print('\\nTest set: Average Loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct/len(test_loader.dataset)\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "    nn.Linear(784,128),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(128,64),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(64,10),\n",
    "    nn.LogSoftmax(dim=1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runSingle(args_, model_, train_loader, test_loader):\n",
    "\n",
    "    for epoch in range(1, args_.epochs+1):\n",
    "        trainSingle(args_, model_, train_loader, epoch)\n",
    "        test(args_, model_, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Train Epoch: 1 [0/60032 (0%)]\tLoss: 2.306031\nTrain Epoch: 1 [1920/60032 (3%)]\tLoss: 2.280874\nTrain Epoch: 1 [3840/60032 (6%)]\tLoss: 2.274052\nTrain Epoch: 1 [5760/60032 (10%)]\tLoss: 2.264879\nTrain Epoch: 1 [7680/60032 (13%)]\tLoss: 2.240621\nTrain Epoch: 1 [9600/60032 (16%)]\tLoss: 2.221420\nTrain Epoch: 1 [11520/60032 (19%)]\tLoss: 2.211297\nTrain Epoch: 1 [13440/60032 (22%)]\tLoss: 2.163599\nTrain Epoch: 1 [15360/60032 (26%)]\tLoss: 2.132312\nTrain Epoch: 1 [17280/60032 (29%)]\tLoss: 2.106156\nTrain Epoch: 1 [19200/60032 (32%)]\tLoss: 2.092826\nTrain Epoch: 1 [21120/60032 (35%)]\tLoss: 2.065591\nTrain Epoch: 1 [23040/60032 (38%)]\tLoss: 2.063183\nTrain Epoch: 1 [24960/60032 (42%)]\tLoss: 1.968164\nTrain Epoch: 1 [26880/60032 (45%)]\tLoss: 1.965822\nTrain Epoch: 1 [28800/60032 (48%)]\tLoss: 1.942527\nTrain Epoch: 1 [30720/60032 (51%)]\tLoss: 1.929292\nTrain Epoch: 1 [32640/60032 (54%)]\tLoss: 1.907287\nTrain Epoch: 1 [34560/60032 (58%)]\tLoss: 1.704797\nTrain Epoch: 1 [36480/60032 (61%)]\tLoss: 1.786498\nTrain Epoch: 1 [38400/60032 (64%)]\tLoss: 1.772372\nTrain Epoch: 1 [40320/60032 (67%)]\tLoss: 1.601672\nTrain Epoch: 1 [42240/60032 (70%)]\tLoss: 1.685461\nTrain Epoch: 1 [44160/60032 (74%)]\tLoss: 1.566831\nTrain Epoch: 1 [46080/60032 (77%)]\tLoss: 1.518300\nTrain Epoch: 1 [48000/60032 (80%)]\tLoss: 1.503881\nTrain Epoch: 1 [49920/60032 (83%)]\tLoss: 1.445182\nTrain Epoch: 1 [51840/60032 (86%)]\tLoss: 1.376610\nTrain Epoch: 1 [53760/60032 (90%)]\tLoss: 1.290247\nTrain Epoch: 1 [55680/60032 (93%)]\tLoss: 1.389650\nTrain Epoch: 1 [57600/60032 (96%)]\tLoss: 1.220942\nTrain Epoch: 1 [59520/60032 (99%)]\tLoss: 1.318542\n\nTest set: Average Loss: 0.0188, Accuracy: 7316/10000 (73%)\n\nTrain Epoch: 2 [0/60032 (0%)]\tLoss: 1.106308\nTrain Epoch: 2 [1920/60032 (3%)]\tLoss: 1.146695\nTrain Epoch: 2 [3840/60032 (6%)]\tLoss: 1.007925\nTrain Epoch: 2 [5760/60032 (10%)]\tLoss: 0.995079\nTrain Epoch: 2 [7680/60032 (13%)]\tLoss: 1.066572\nTrain Epoch: 2 [9600/60032 (16%)]\tLoss: 1.096259\nTrain Epoch: 2 [11520/60032 (19%)]\tLoss: 1.003737\nTrain Epoch: 2 [13440/60032 (22%)]\tLoss: 0.938146\nTrain Epoch: 2 [15360/60032 (26%)]\tLoss: 1.100113\nTrain Epoch: 2 [17280/60032 (29%)]\tLoss: 0.898534\nTrain Epoch: 2 [19200/60032 (32%)]\tLoss: 0.873275\nTrain Epoch: 2 [21120/60032 (35%)]\tLoss: 0.808253\nTrain Epoch: 2 [23040/60032 (38%)]\tLoss: 0.831082\nTrain Epoch: 2 [24960/60032 (42%)]\tLoss: 0.699029\nTrain Epoch: 2 [26880/60032 (45%)]\tLoss: 0.875549\nTrain Epoch: 2 [28800/60032 (48%)]\tLoss: 0.841717\nTrain Epoch: 2 [30720/60032 (51%)]\tLoss: 0.828364\nTrain Epoch: 2 [32640/60032 (54%)]\tLoss: 0.810686\nTrain Epoch: 2 [34560/60032 (58%)]\tLoss: 0.773038\nTrain Epoch: 2 [36480/60032 (61%)]\tLoss: 0.894853\nTrain Epoch: 2 [38400/60032 (64%)]\tLoss: 0.758783\nTrain Epoch: 2 [40320/60032 (67%)]\tLoss: 0.896506\nTrain Epoch: 2 [42240/60032 (70%)]\tLoss: 0.701333\nTrain Epoch: 2 [44160/60032 (74%)]\tLoss: 0.820921\nTrain Epoch: 2 [46080/60032 (77%)]\tLoss: 0.602848\nTrain Epoch: 2 [48000/60032 (80%)]\tLoss: 0.682383\nTrain Epoch: 2 [49920/60032 (83%)]\tLoss: 0.620668\nTrain Epoch: 2 [51840/60032 (86%)]\tLoss: 0.564021\nTrain Epoch: 2 [53760/60032 (90%)]\tLoss: 0.675007\nTrain Epoch: 2 [55680/60032 (93%)]\tLoss: 0.609954\nTrain Epoch: 2 [57600/60032 (96%)]\tLoss: 0.578960\nTrain Epoch: 2 [59520/60032 (99%)]\tLoss: 0.544971\n\nTest set: Average Loss: 0.0094, Accuracy: 8413/10000 (84%)\n\nTrain Epoch: 3 [0/60032 (0%)]\tLoss: 0.544109\nTrain Epoch: 3 [1920/60032 (3%)]\tLoss: 0.437579\nTrain Epoch: 3 [3840/60032 (6%)]\tLoss: 0.457540\nTrain Epoch: 3 [5760/60032 (10%)]\tLoss: 0.619553\nTrain Epoch: 3 [7680/60032 (13%)]\tLoss: 0.813851\nTrain Epoch: 3 [9600/60032 (16%)]\tLoss: 0.664103\nTrain Epoch: 3 [11520/60032 (19%)]\tLoss: 0.617046\nTrain Epoch: 3 [13440/60032 (22%)]\tLoss: 0.533043\nTrain Epoch: 3 [15360/60032 (26%)]\tLoss: 0.527679\nTrain Epoch: 3 [17280/60032 (29%)]\tLoss: 0.519457\nTrain Epoch: 3 [19200/60032 (32%)]\tLoss: 0.462820\nTrain Epoch: 3 [21120/60032 (35%)]\tLoss: 0.484768\nTrain Epoch: 3 [23040/60032 (38%)]\tLoss: 0.672576\nTrain Epoch: 3 [24960/60032 (42%)]\tLoss: 0.560777\nTrain Epoch: 3 [26880/60032 (45%)]\tLoss: 0.623415\nTrain Epoch: 3 [28800/60032 (48%)]\tLoss: 0.416605\nTrain Epoch: 3 [30720/60032 (51%)]\tLoss: 0.429821\nTrain Epoch: 3 [32640/60032 (54%)]\tLoss: 0.530973\nTrain Epoch: 3 [34560/60032 (58%)]\tLoss: 0.598084\nTrain Epoch: 3 [36480/60032 (61%)]\tLoss: 0.512866\nTrain Epoch: 3 [38400/60032 (64%)]\tLoss: 0.405358\nTrain Epoch: 3 [40320/60032 (67%)]\tLoss: 0.506308\nTrain Epoch: 3 [42240/60032 (70%)]\tLoss: 0.338071\nTrain Epoch: 3 [44160/60032 (74%)]\tLoss: 0.699210\nTrain Epoch: 3 [46080/60032 (77%)]\tLoss: 0.549106\nTrain Epoch: 3 [48000/60032 (80%)]\tLoss: 0.461305\nTrain Epoch: 3 [49920/60032 (83%)]\tLoss: 0.520584\nTrain Epoch: 3 [51840/60032 (86%)]\tLoss: 0.468074\nTrain Epoch: 3 [53760/60032 (90%)]\tLoss: 0.423048\nTrain Epoch: 3 [55680/60032 (93%)]\tLoss: 0.252415\nTrain Epoch: 3 [57600/60032 (96%)]\tLoss: 0.423915\nTrain Epoch: 3 [59520/60032 (99%)]\tLoss: 0.592304\n\nTest set: Average Loss: 0.0071, Accuracy: 8754/10000 (88%)\n\nTrain Epoch: 4 [0/60032 (0%)]\tLoss: 0.422049\nTrain Epoch: 4 [1920/60032 (3%)]\tLoss: 0.662024\nTrain Epoch: 4 [3840/60032 (6%)]\tLoss: 0.482346\nTrain Epoch: 4 [5760/60032 (10%)]\tLoss: 0.495271\nTrain Epoch: 4 [7680/60032 (13%)]\tLoss: 0.361252\nTrain Epoch: 4 [9600/60032 (16%)]\tLoss: 0.284881\nTrain Epoch: 4 [11520/60032 (19%)]\tLoss: 0.372806\nTrain Epoch: 4 [13440/60032 (22%)]\tLoss: 0.332291\nTrain Epoch: 4 [15360/60032 (26%)]\tLoss: 0.504094\nTrain Epoch: 4 [17280/60032 (29%)]\tLoss: 0.482805\nTrain Epoch: 4 [19200/60032 (32%)]\tLoss: 0.571814\nTrain Epoch: 4 [21120/60032 (35%)]\tLoss: 0.522284\nTrain Epoch: 4 [23040/60032 (38%)]\tLoss: 0.496400\nTrain Epoch: 4 [24960/60032 (42%)]\tLoss: 0.567269\nTrain Epoch: 4 [26880/60032 (45%)]\tLoss: 0.468202\nTrain Epoch: 4 [28800/60032 (48%)]\tLoss: 0.521695\nTrain Epoch: 4 [30720/60032 (51%)]\tLoss: 0.279708\nTrain Epoch: 4 [32640/60032 (54%)]\tLoss: 0.346418\nTrain Epoch: 4 [34560/60032 (58%)]\tLoss: 0.377722\nTrain Epoch: 4 [36480/60032 (61%)]\tLoss: 0.322156\nTrain Epoch: 4 [38400/60032 (64%)]\tLoss: 0.378404\nTrain Epoch: 4 [40320/60032 (67%)]\tLoss: 0.401125\nTrain Epoch: 4 [42240/60032 (70%)]\tLoss: 0.579517\nTrain Epoch: 4 [44160/60032 (74%)]\tLoss: 0.339255\nTrain Epoch: 4 [46080/60032 (77%)]\tLoss: 0.294588\nTrain Epoch: 4 [48000/60032 (80%)]\tLoss: 0.643138\nTrain Epoch: 4 [49920/60032 (83%)]\tLoss: 0.374297\nTrain Epoch: 4 [51840/60032 (86%)]\tLoss: 0.436434\nTrain Epoch: 4 [53760/60032 (90%)]\tLoss: 0.395996\nTrain Epoch: 4 [55680/60032 (93%)]\tLoss: 0.369875\nTrain Epoch: 4 [57600/60032 (96%)]\tLoss: 0.434441\nTrain Epoch: 4 [59520/60032 (99%)]\tLoss: 0.472525\n\nTest set: Average Loss: 0.0061, Accuracy: 8898/10000 (89%)\n\nTrain Epoch: 5 [0/60032 (0%)]\tLoss: 0.254365\nTrain Epoch: 5 [1920/60032 (3%)]\tLoss: 0.398275\nTrain Epoch: 5 [3840/60032 (6%)]\tLoss: 0.441474\nTrain Epoch: 5 [5760/60032 (10%)]\tLoss: 0.446091\nTrain Epoch: 5 [7680/60032 (13%)]\tLoss: 0.479927\nTrain Epoch: 5 [9600/60032 (16%)]\tLoss: 0.270601\nTrain Epoch: 5 [11520/60032 (19%)]\tLoss: 0.374118\nTrain Epoch: 5 [13440/60032 (22%)]\tLoss: 0.467563\nTrain Epoch: 5 [15360/60032 (26%)]\tLoss: 0.270795\nTrain Epoch: 5 [17280/60032 (29%)]\tLoss: 0.340215\nTrain Epoch: 5 [19200/60032 (32%)]\tLoss: 0.385649\nTrain Epoch: 5 [21120/60032 (35%)]\tLoss: 0.386040\nTrain Epoch: 5 [23040/60032 (38%)]\tLoss: 0.444103\nTrain Epoch: 5 [24960/60032 (42%)]\tLoss: 0.364324\nTrain Epoch: 5 [26880/60032 (45%)]\tLoss: 0.319513\nTrain Epoch: 5 [28800/60032 (48%)]\tLoss: 0.446993\nTrain Epoch: 5 [30720/60032 (51%)]\tLoss: 0.538390\nTrain Epoch: 5 [32640/60032 (54%)]\tLoss: 0.417085\nTrain Epoch: 5 [34560/60032 (58%)]\tLoss: 0.484774\nTrain Epoch: 5 [36480/60032 (61%)]\tLoss: 0.237710\nTrain Epoch: 5 [38400/60032 (64%)]\tLoss: 0.389457\nTrain Epoch: 5 [40320/60032 (67%)]\tLoss: 0.392733\nTrain Epoch: 5 [42240/60032 (70%)]\tLoss: 0.487593\nTrain Epoch: 5 [44160/60032 (74%)]\tLoss: 0.239483\nTrain Epoch: 5 [46080/60032 (77%)]\tLoss: 0.458545\nTrain Epoch: 5 [48000/60032 (80%)]\tLoss: 0.215909\nTrain Epoch: 5 [49920/60032 (83%)]\tLoss: 0.248660\nTrain Epoch: 5 [51840/60032 (86%)]\tLoss: 0.425929\nTrain Epoch: 5 [53760/60032 (90%)]\tLoss: 0.469256\nTrain Epoch: 5 [55680/60032 (93%)]\tLoss: 0.320111\nTrain Epoch: 5 [57600/60032 (96%)]\tLoss: 0.405901\nTrain Epoch: 5 [59520/60032 (99%)]\tLoss: 0.270446\n\nTest set: Average Loss: 0.0056, Accuracy: 8963/10000 (90%)\n\n"
    }
   ],
   "source": [
    "runSingle(args, model, trainloader, testloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for federated learning we need to create federatedlaoder\n",
    "# but first need to define machines\n",
    "import syft as sy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "hook = sy.TorchHook(th)\n",
    "\n",
    "bob = sy.VirtualWorker(hook, id='bob')\n",
    "alice = sy.VirtualWorker(hook, id='alice')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "federated_train_loader = sy.FederatedDataLoader( # <-- this is now a FederatedDataLoader \n",
    "    datasets.MNIST('./data', train=True, download=True,\n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.5,), (0.5,))\n",
    "                   ]))\n",
    "    .federate((bob, alice)), # <-- NEW: we distribute the dataset across all the workers, it's now a FederatedDataset\n",
    "    batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainFL(args, model, federated_train_loader, epoch):\n",
    "    \n",
    "    criterion = nn.NLLLoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.003)\n",
    "\n",
    "\n",
    "    for batch_idx, (data, target) in enumerate(federated_train_loader): # <-- now it is a distributed dataset\n",
    "        model.send(data.location) # <-- NEW: send the model to the right location        \n",
    "        #data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data.view(data.shape[0], -1))\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        model.get() # <-- NEW: get the model back\n",
    "        \n",
    "        if batch_idx % args.log_interval == 0:\n",
    "            loss = loss.get() # <-- NEW: get the loss back\n",
    "            print('Location: -->{}'.format(data.location))\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * args.batch_size, len(federated_train_loader) * args.batch_size,\n",
    "                100. * batch_idx / len(federated_train_loader), loss.item()\n",
    "                ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runFL(args_, model_, federated_train_loader, test_loader):\n",
    "\n",
    "    for epoch in range(1, args_.epochs+1):\n",
    "        trainFL(args_, model_, federated_train_loader, epoch)\n",
    "        test(args_, model_, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Location: --><VirtualWorker id:bob #objects:5>\nTrain Epoch: 1 [0/60032 (0%)]\tLoss: 0.363805\nLocation: --><VirtualWorker id:bob #objects:5>\nTrain Epoch: 1 [1920/60032 (3%)]\tLoss: 0.376995\nLocation: --><VirtualWorker id:bob #objects:5>\nTrain Epoch: 1 [3840/60032 (6%)]\tLoss: 0.451310\nLocation: --><VirtualWorker id:bob #objects:5>\nTrain Epoch: 1 [5760/60032 (10%)]\tLoss: 0.279799\nLocation: --><VirtualWorker id:bob #objects:5>\nTrain Epoch: 1 [7680/60032 (13%)]\tLoss: 0.537798\nLocation: --><VirtualWorker id:bob #objects:5>\nTrain Epoch: 1 [9600/60032 (16%)]\tLoss: 0.318719\nLocation: --><VirtualWorker id:bob #objects:5>\nTrain Epoch: 1 [11520/60032 (19%)]\tLoss: 0.240266\nLocation: --><VirtualWorker id:bob #objects:5>\nTrain Epoch: 1 [13440/60032 (22%)]\tLoss: 0.379196\nLocation: --><VirtualWorker id:bob #objects:5>\nTrain Epoch: 1 [15360/60032 (26%)]\tLoss: 0.345611\nLocation: --><VirtualWorker id:bob #objects:5>\nTrain Epoch: 1 [17280/60032 (29%)]\tLoss: 0.261623\nLocation: --><VirtualWorker id:bob #objects:5>\nTrain Epoch: 1 [19200/60032 (32%)]\tLoss: 0.272756\nLocation: --><VirtualWorker id:bob #objects:5>\nTrain Epoch: 1 [21120/60032 (35%)]\tLoss: 0.447991\nLocation: --><VirtualWorker id:bob #objects:5>\nTrain Epoch: 1 [23040/60032 (38%)]\tLoss: 0.327801\nLocation: --><VirtualWorker id:bob #objects:5>\nTrain Epoch: 1 [24960/60032 (42%)]\tLoss: 0.393936\nLocation: --><VirtualWorker id:bob #objects:5>\nTrain Epoch: 1 [26880/60032 (45%)]\tLoss: 0.310394\nLocation: --><VirtualWorker id:bob #objects:5>\nTrain Epoch: 1 [28800/60032 (48%)]\tLoss: 0.289115\nLocation: --><VirtualWorker id:alice #objects:5>\nTrain Epoch: 1 [30720/60032 (51%)]\tLoss: 0.508820\nLocation: --><VirtualWorker id:alice #objects:5>\nTrain Epoch: 1 [32640/60032 (54%)]\tLoss: 0.230688\nLocation: --><VirtualWorker id:alice #objects:5>\nTrain Epoch: 1 [34560/60032 (58%)]\tLoss: 0.325574\nLocation: --><VirtualWorker id:alice #objects:5>\nTrain Epoch: 1 [36480/60032 (61%)]\tLoss: 0.335627\nLocation: --><VirtualWorker id:alice #objects:5>\nTrain Epoch: 1 [38400/60032 (64%)]\tLoss: 0.298178\nLocation: --><VirtualWorker id:alice #objects:5>\nTrain Epoch: 1 [40320/60032 (67%)]\tLoss: 0.422718\nLocation: --><VirtualWorker id:alice #objects:5>\nTrain Epoch: 1 [42240/60032 (70%)]\tLoss: 0.271037\nLocation: --><VirtualWorker id:alice #objects:5>\nTrain Epoch: 1 [44160/60032 (74%)]\tLoss: 0.343790\nLocation: --><VirtualWorker id:alice #objects:5>\nTrain Epoch: 1 [46080/60032 (77%)]\tLoss: 0.496378\nLocation: --><VirtualWorker id:alice #objects:5>\nTrain Epoch: 1 [48000/60032 (80%)]\tLoss: 0.348094\nLocation: --><VirtualWorker id:alice #objects:5>\nTrain Epoch: 1 [49920/60032 (83%)]\tLoss: 0.445818\nLocation: --><VirtualWorker id:alice #objects:5>\nTrain Epoch: 1 [51840/60032 (86%)]\tLoss: 0.391079\nLocation: --><VirtualWorker id:alice #objects:5>\nTrain Epoch: 1 [53760/60032 (90%)]\tLoss: 0.371614\nLocation: --><VirtualWorker id:alice #objects:5>\nTrain Epoch: 1 [55680/60032 (93%)]\tLoss: 0.471932\nLocation: --><VirtualWorker id:alice #objects:5>\nTrain Epoch: 1 [57600/60032 (96%)]\tLoss: 0.416183\nLocation: --><VirtualWorker id:alice #objects:5>\nTrain Epoch: 1 [59520/60032 (99%)]\tLoss: 0.336268\n\nTest set: Average Loss: 0.0052, Accuracy: 9026/10000 (90%)\n\nLocation: --><VirtualWorker id:bob #objects:5>\nTrain Epoch: 2 [0/60032 (0%)]\tLoss: 0.274777\nLocation: --><VirtualWorker id:bob #objects:5>\nTrain Epoch: 2 [1920/60032 (3%)]\tLoss: 0.244703\nLocation: --><VirtualWorker id:bob #objects:5>\nTrain Epoch: 2 [3840/60032 (6%)]\tLoss: 0.354385\nLocation: --><VirtualWorker id:bob #objects:5>\nTrain Epoch: 2 [5760/60032 (10%)]\tLoss: 0.195572\nLocation: --><VirtualWorker id:bob #objects:5>\nTrain Epoch: 2 [7680/60032 (13%)]\tLoss: 0.353994\nLocation: --><VirtualWorker id:bob #objects:5>\nTrain Epoch: 2 [9600/60032 (16%)]\tLoss: 0.610215\nLocation: --><VirtualWorker id:bob #objects:5>\nTrain Epoch: 2 [11520/60032 (19%)]\tLoss: 0.463473\nLocation: --><VirtualWorker id:bob #objects:5>\nTrain Epoch: 2 [13440/60032 (22%)]\tLoss: 0.478605\nLocation: --><VirtualWorker id:bob #objects:5>\nTrain Epoch: 2 [15360/60032 (26%)]\tLoss: 0.330962\nLocation: --><VirtualWorker id:bob #objects:5>\nTrain Epoch: 2 [17280/60032 (29%)]\tLoss: 0.160155\nLocation: --><VirtualWorker id:bob #objects:5>\nTrain Epoch: 2 [19200/60032 (32%)]\tLoss: 0.146145\nLocation: --><VirtualWorker id:bob #objects:5>\nTrain Epoch: 2 [21120/60032 (35%)]\tLoss: 0.509873\nLocation: --><VirtualWorker id:bob #objects:5>\nTrain Epoch: 2 [23040/60032 (38%)]\tLoss: 0.486954\nLocation: --><VirtualWorker id:bob #objects:5>\nTrain Epoch: 2 [24960/60032 (42%)]\tLoss: 0.188033\nLocation: --><VirtualWorker id:bob #objects:5>\nTrain Epoch: 2 [26880/60032 (45%)]\tLoss: 0.321393\nLocation: --><VirtualWorker id:bob #objects:5>\nTrain Epoch: 2 [28800/60032 (48%)]\tLoss: 0.356624\nLocation: --><VirtualWorker id:alice #objects:5>\nTrain Epoch: 2 [30720/60032 (51%)]\tLoss: 0.213932\nLocation: --><VirtualWorker id:alice #objects:5>\nTrain Epoch: 2 [32640/60032 (54%)]\tLoss: 0.342829\nLocation: --><VirtualWorker id:alice #objects:5>\nTrain Epoch: 2 [34560/60032 (58%)]\tLoss: 0.363843\nLocation: --><VirtualWorker id:alice #objects:5>\nTrain Epoch: 2 [36480/60032 (61%)]\tLoss: 0.512654\nLocation: --><VirtualWorker id:alice #objects:5>\nTrain Epoch: 2 [38400/60032 (64%)]\tLoss: 0.349208\nLocation: --><VirtualWorker id:alice #objects:5>\nTrain Epoch: 2 [40320/60032 (67%)]\tLoss: 0.308709\nLocation: --><VirtualWorker id:alice #objects:5>\nTrain Epoch: 2 [42240/60032 (70%)]\tLoss: 0.367568\nLocation: --><VirtualWorker id:alice #objects:5>\nTrain Epoch: 2 [44160/60032 (74%)]\tLoss: 0.267759\nLocation: --><VirtualWorker id:alice #objects:5>\nTrain Epoch: 2 [46080/60032 (77%)]\tLoss: 0.362104\nLocation: --><VirtualWorker id:alice #objects:5>\nTrain Epoch: 2 [48000/60032 (80%)]\tLoss: 0.634820\nLocation: --><VirtualWorker id:alice #objects:5>\nTrain Epoch: 2 [49920/60032 (83%)]\tLoss: 0.232358\nLocation: --><VirtualWorker id:alice #objects:5>\nTrain Epoch: 2 [51840/60032 (86%)]\tLoss: 0.379622\nLocation: --><VirtualWorker id:alice #objects:5>\nTrain Epoch: 2 [53760/60032 (90%)]\tLoss: 0.213159\nLocation: --><VirtualWorker id:alice #objects:5>\nTrain Epoch: 2 [55680/60032 (93%)]\tLoss: 0.314923\nLocation: --><VirtualWorker id:alice #objects:5>\nTrain Epoch: 2 [57600/60032 (96%)]\tLoss: 0.289358\nLocation: --><VirtualWorker id:alice #objects:5>\nTrain Epoch: 2 [59520/60032 (99%)]\tLoss: 0.291729\n\nTest set: Average Loss: 0.0050, Accuracy: 9092/10000 (91%)\n\nLocation: --><VirtualWorker id:bob #objects:5>\nTrain Epoch: 3 [0/60032 (0%)]\tLoss: 0.287837\nLocation: --><VirtualWorker id:bob #objects:5>\nTrain Epoch: 3 [1920/60032 (3%)]\tLoss: 0.231594\nLocation: --><VirtualWorker id:bob #objects:5>\nTrain Epoch: 3 [3840/60032 (6%)]\tLoss: 0.460810\nLocation: --><VirtualWorker id:bob #objects:5>\nTrain Epoch: 3 [5760/60032 (10%)]\tLoss: 0.293472\nLocation: --><VirtualWorker id:bob #objects:5>\nTrain Epoch: 3 [7680/60032 (13%)]\tLoss: 0.364226\nLocation: --><VirtualWorker id:bob #objects:5>\nTrain Epoch: 3 [9600/60032 (16%)]\tLoss: 0.552987\nLocation: --><VirtualWorker id:bob #objects:5>\nTrain Epoch: 3 [11520/60032 (19%)]\tLoss: 0.228543\nLocation: --><VirtualWorker id:bob #objects:5>\nTrain Epoch: 3 [13440/60032 (22%)]\tLoss: 0.216944\nLocation: --><VirtualWorker id:bob #objects:5>\nTrain Epoch: 3 [15360/60032 (26%)]\tLoss: 0.286462\nLocation: --><VirtualWorker id:bob #objects:5>\nTrain Epoch: 3 [17280/60032 (29%)]\tLoss: 0.344376\nLocation: --><VirtualWorker id:bob #objects:5>\nTrain Epoch: 3 [19200/60032 (32%)]\tLoss: 0.221038\nLocation: --><VirtualWorker id:bob #objects:5>\nTrain Epoch: 3 [21120/60032 (35%)]\tLoss: 0.231550\nLocation: --><VirtualWorker id:bob #objects:5>\nTrain Epoch: 3 [23040/60032 (38%)]\tLoss: 0.321770\nLocation: --><VirtualWorker id:bob #objects:5>\nTrain Epoch: 3 [24960/60032 (42%)]\tLoss: 0.418243\nLocation: --><VirtualWorker id:bob #objects:5>\nTrain Epoch: 3 [26880/60032 (45%)]\tLoss: 0.261858\nLocation: --><VirtualWorker id:bob #objects:5>\nTrain Epoch: 3 [28800/60032 (48%)]\tLoss: 0.519660\nLocation: --><VirtualWorker id:alice #objects:5>\nTrain Epoch: 3 [30720/60032 (51%)]\tLoss: 0.422102\nLocation: --><VirtualWorker id:alice #objects:5>\nTrain Epoch: 3 [32640/60032 (54%)]\tLoss: 0.248533\nLocation: --><VirtualWorker id:alice #objects:5>\nTrain Epoch: 3 [34560/60032 (58%)]\tLoss: 0.254622\nLocation: --><VirtualWorker id:alice #objects:5>\nTrain Epoch: 3 [36480/60032 (61%)]\tLoss: 0.268586\nLocation: --><VirtualWorker id:alice #objects:5>\nTrain Epoch: 3 [38400/60032 (64%)]\tLoss: 0.284727\nLocation: --><VirtualWorker id:alice #objects:5>\nTrain Epoch: 3 [40320/60032 (67%)]\tLoss: 0.360055\nLocation: --><VirtualWorker id:alice #objects:5>\nTrain Epoch: 3 [42240/60032 (70%)]\tLoss: 0.352683\nLocation: --><VirtualWorker id:alice #objects:5>\nTrain Epoch: 3 [44160/60032 (74%)]\tLoss: 0.269969\nLocation: --><VirtualWorker id:alice #objects:5>\nTrain Epoch: 3 [46080/60032 (77%)]\tLoss: 0.270055\nLocation: --><VirtualWorker id:alice #objects:5>\nTrain Epoch: 3 [48000/60032 (80%)]\tLoss: 0.366237\nLocation: --><VirtualWorker id:alice #objects:5>\nTrain Epoch: 3 [49920/60032 (83%)]\tLoss: 0.324315\nLocation: --><VirtualWorker id:alice #objects:5>\nTrain Epoch: 3 [51840/60032 (86%)]\tLoss: 0.351176\nLocation: --><VirtualWorker id:alice #objects:5>\nTrain Epoch: 3 [53760/60032 (90%)]\tLoss: 0.173738\nLocation: --><VirtualWorker id:alice #objects:5>\nTrain Epoch: 3 [55680/60032 (93%)]\tLoss: 0.351380\nLocation: --><VirtualWorker id:alice #objects:5>\nTrain Epoch: 3 [57600/60032 (96%)]\tLoss: 0.443319\nLocation: --><VirtualWorker id:alice #objects:5>\nTrain Epoch: 3 [59520/60032 (99%)]\tLoss: 0.205116\n\nTest set: Average Loss: 0.0048, Accuracy: 9098/10000 (91%)\n\nLocation: --><VirtualWorker id:bob #objects:5>\nTrain Epoch: 4 [0/60032 (0%)]\tLoss: 0.158731\nLocation: --><VirtualWorker id:bob #objects:5>\nTrain Epoch: 4 [1920/60032 (3%)]\tLoss: 0.223227\nLocation: --><VirtualWorker id:bob #objects:5>\nTrain Epoch: 4 [3840/60032 (6%)]\tLoss: 0.324120\nLocation: --><VirtualWorker id:bob #objects:5>\nTrain Epoch: 4 [5760/60032 (10%)]\tLoss: 0.173363\nLocation: --><VirtualWorker id:bob #objects:5>\nTrain Epoch: 4 [7680/60032 (13%)]\tLoss: 0.148039\nLocation: --><VirtualWorker id:bob #objects:5>\nTrain Epoch: 4 [9600/60032 (16%)]\tLoss: 0.294466\nLocation: --><VirtualWorker id:bob #objects:5>\nTrain Epoch: 4 [11520/60032 (19%)]\tLoss: 0.224487\nLocation: --><VirtualWorker id:bob #objects:5>\nTrain Epoch: 4 [13440/60032 (22%)]\tLoss: 0.367842\nLocation: --><VirtualWorker id:bob #objects:5>\nTrain Epoch: 4 [15360/60032 (26%)]\tLoss: 0.265380\nLocation: --><VirtualWorker id:bob #objects:5>\nTrain Epoch: 4 [17280/60032 (29%)]\tLoss: 0.352050\nLocation: --><VirtualWorker id:bob #objects:5>\nTrain Epoch: 4 [19200/60032 (32%)]\tLoss: 0.620799\nLocation: --><VirtualWorker id:bob #objects:5>\nTrain Epoch: 4 [21120/60032 (35%)]\tLoss: 0.315400\nLocation: --><VirtualWorker id:bob #objects:5>\nTrain Epoch: 4 [23040/60032 (38%)]\tLoss: 0.422055\nLocation: --><VirtualWorker id:bob #objects:5>\nTrain Epoch: 4 [24960/60032 (42%)]\tLoss: 0.319243\nLocation: --><VirtualWorker id:bob #objects:5>\nTrain Epoch: 4 [26880/60032 (45%)]\tLoss: 0.362057\nLocation: --><VirtualWorker id:bob #objects:5>\nTrain Epoch: 4 [28800/60032 (48%)]\tLoss: 0.356943\nLocation: --><VirtualWorker id:alice #objects:5>\nTrain Epoch: 4 [30720/60032 (51%)]\tLoss: 0.308845\nLocation: --><VirtualWorker id:alice #objects:5>\nTrain Epoch: 4 [32640/60032 (54%)]\tLoss: 0.476607\nLocation: --><VirtualWorker id:alice #objects:5>\nTrain Epoch: 4 [34560/60032 (58%)]\tLoss: 0.264955\nLocation: --><VirtualWorker id:alice #objects:5>\nTrain Epoch: 4 [36480/60032 (61%)]\tLoss: 0.359045\nLocation: --><VirtualWorker id:alice #objects:5>\nTrain Epoch: 4 [38400/60032 (64%)]\tLoss: 0.245640\nLocation: --><VirtualWorker id:alice #objects:5>\nTrain Epoch: 4 [40320/60032 (67%)]\tLoss: 0.431467\nLocation: --><VirtualWorker id:alice #objects:5>\nTrain Epoch: 4 [42240/60032 (70%)]\tLoss: 0.434288\nLocation: --><VirtualWorker id:alice #objects:5>\nTrain Epoch: 4 [44160/60032 (74%)]\tLoss: 0.461495\nLocation: --><VirtualWorker id:alice #objects:5>\nTrain Epoch: 4 [46080/60032 (77%)]\tLoss: 0.310967\nLocation: --><VirtualWorker id:alice #objects:5>\nTrain Epoch: 4 [48000/60032 (80%)]\tLoss: 0.442459\nLocation: --><VirtualWorker id:alice #objects:5>\nTrain Epoch: 4 [49920/60032 (83%)]\tLoss: 0.448954\nLocation: --><VirtualWorker id:alice #objects:5>\nTrain Epoch: 4 [51840/60032 (86%)]\tLoss: 0.299043\nLocation: --><VirtualWorker id:alice #objects:5>\nTrain Epoch: 4 [53760/60032 (90%)]\tLoss: 0.268696\nLocation: --><VirtualWorker id:alice #objects:5>\nTrain Epoch: 4 [55680/60032 (93%)]\tLoss: 0.341781\nLocation: --><VirtualWorker id:alice #objects:5>\nTrain Epoch: 4 [57600/60032 (96%)]\tLoss: 0.209968\nLocation: --><VirtualWorker id:alice #objects:5>\nTrain Epoch: 4 [59520/60032 (99%)]\tLoss: 0.495684\n\nTest set: Average Loss: 0.0046, Accuracy: 9147/10000 (91%)\n\nLocation: --><VirtualWorker id:bob #objects:5>\nTrain Epoch: 5 [0/60032 (0%)]\tLoss: 0.281154\nLocation: --><VirtualWorker id:bob #objects:5>\nTrain Epoch: 5 [1920/60032 (3%)]\tLoss: 0.276127\nLocation: --><VirtualWorker id:bob #objects:5>\nTrain Epoch: 5 [3840/60032 (6%)]\tLoss: 0.388120\nLocation: --><VirtualWorker id:bob #objects:5>\nTrain Epoch: 5 [5760/60032 (10%)]\tLoss: 0.316342\nLocation: --><VirtualWorker id:bob #objects:5>\nTrain Epoch: 5 [7680/60032 (13%)]\tLoss: 0.187895\nLocation: --><VirtualWorker id:bob #objects:5>\nTrain Epoch: 5 [9600/60032 (16%)]\tLoss: 0.385566\nLocation: --><VirtualWorker id:bob #objects:5>\nTrain Epoch: 5 [11520/60032 (19%)]\tLoss: 0.334518\nLocation: --><VirtualWorker id:bob #objects:5>\nTrain Epoch: 5 [13440/60032 (22%)]\tLoss: 0.213058\nLocation: --><VirtualWorker id:bob #objects:5>\nTrain Epoch: 5 [15360/60032 (26%)]\tLoss: 0.206899\nLocation: --><VirtualWorker id:bob #objects:5>\nTrain Epoch: 5 [17280/60032 (29%)]\tLoss: 0.240024\nLocation: --><VirtualWorker id:bob #objects:5>\nTrain Epoch: 5 [19200/60032 (32%)]\tLoss: 0.255461\nLocation: --><VirtualWorker id:bob #objects:5>\nTrain Epoch: 5 [21120/60032 (35%)]\tLoss: 0.183220\nLocation: --><VirtualWorker id:bob #objects:5>\nTrain Epoch: 5 [23040/60032 (38%)]\tLoss: 0.309582\nLocation: --><VirtualWorker id:bob #objects:5>\nTrain Epoch: 5 [24960/60032 (42%)]\tLoss: 0.271590\nLocation: --><VirtualWorker id:bob #objects:5>\nTrain Epoch: 5 [26880/60032 (45%)]\tLoss: 0.193253\nLocation: --><VirtualWorker id:bob #objects:5>\nTrain Epoch: 5 [28800/60032 (48%)]\tLoss: 0.220127\nLocation: --><VirtualWorker id:alice #objects:5>\nTrain Epoch: 5 [30720/60032 (51%)]\tLoss: 0.295242\nLocation: --><VirtualWorker id:alice #objects:5>\nTrain Epoch: 5 [32640/60032 (54%)]\tLoss: 0.346748\nLocation: --><VirtualWorker id:alice #objects:5>\nTrain Epoch: 5 [34560/60032 (58%)]\tLoss: 0.354922\nLocation: --><VirtualWorker id:alice #objects:5>\nTrain Epoch: 5 [36480/60032 (61%)]\tLoss: 0.285971\nLocation: --><VirtualWorker id:alice #objects:5>\nTrain Epoch: 5 [38400/60032 (64%)]\tLoss: 0.336619\nLocation: --><VirtualWorker id:alice #objects:5>\nTrain Epoch: 5 [40320/60032 (67%)]\tLoss: 0.324560\nLocation: --><VirtualWorker id:alice #objects:5>\nTrain Epoch: 5 [42240/60032 (70%)]\tLoss: 0.498441\nLocation: --><VirtualWorker id:alice #objects:5>\nTrain Epoch: 5 [44160/60032 (74%)]\tLoss: 0.267318\nLocation: --><VirtualWorker id:alice #objects:5>\nTrain Epoch: 5 [46080/60032 (77%)]\tLoss: 0.427829\nLocation: --><VirtualWorker id:alice #objects:5>\nTrain Epoch: 5 [48000/60032 (80%)]\tLoss: 0.482151\nLocation: --><VirtualWorker id:alice #objects:5>\nTrain Epoch: 5 [49920/60032 (83%)]\tLoss: 0.377704\nLocation: --><VirtualWorker id:alice #objects:5>\nTrain Epoch: 5 [51840/60032 (86%)]\tLoss: 0.341476\nLocation: --><VirtualWorker id:alice #objects:5>\nTrain Epoch: 5 [53760/60032 (90%)]\tLoss: 0.318727\nLocation: --><VirtualWorker id:alice #objects:5>\nTrain Epoch: 5 [55680/60032 (93%)]\tLoss: 0.198703\nLocation: --><VirtualWorker id:alice #objects:5>\nTrain Epoch: 5 [57600/60032 (96%)]\tLoss: 0.149726\nLocation: --><VirtualWorker id:alice #objects:5>\nTrain Epoch: 5 [59520/60032 (99%)]\tLoss: 0.237989\n\nTest set: Average Loss: 0.0045, Accuracy: 9176/10000 (92%)\n\n"
    }
   ],
   "source": [
    "runFL(args, model, federated_train_loader, testloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}